log:
  root_dir: "artifacts/log"
  version: null
  to_file: false
  log_every_n_steps: 5 # expensive computation
  flush_logs_every_n_steps: 10 # expensive computation

chkpt:
  root_dir: "artifacts/checkpoints"
  monitor: "val_loss" # metric to determine if to save
  save_top_k: 1 # only top k models to be save
  every_n_val_epochs: null # save after n (training) epochs. It works if save_top_k > 0
  every_n_train_steps: null # mutually exclusive to every_n_val_epochs
  save_last: true # only work after the first validation step

arcface:
  pth: "artifacts/experiments/idt_encoder/ArcFace.pth"
  vector_size: 256

data:
  trainset_dir: "artifacts/datasets/ffhq/train"
  valset_dir: "artifacts/datasets/ffhq/val"

trainer:
  batch_size: 32 # 32 should work
  num_workers: 2
  max_epoch: 1000
  num_processes: 1 # for distributed computation
  
  val_check_interval: 100

  limit_train_batches: 1000 # restrict a mount of data for training
  limit_val_batches: 100 # equal to the number of data points

model:
  name: "faceshifter"
  experiment: "other_research_code" # to create log and checkpoint dir

  learning_rate_E_G: 4e-4
  learning_rate_D: 4e-4

  beta1: 0
  beta2: 0.999

  grad_clip: 0.0

  