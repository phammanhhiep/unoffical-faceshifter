log:
  root_dir: "artifacts/log"
  version: null
  to_file: false
  log_every_n_steps: 1 # expensive computation; depend on global steps, and thus accumulate_grad_batches
  flush_logs_every_n_steps: 1 # expensive computation; depend on global steps, and thus accumulate_grad_batches
  progress_bar_refresh_rate: 1

chkpt:
  root_dir: "artifacts/checkpoints"
  monitor: "val_loss" # metric to determine if to save
  save_top_k: 1 # only top k models to be save
  every_n_val_epochs: null # save after n (training) epochs. It works if save_top_k > 0
  every_n_train_steps: null # mutually exclusive to every_n_val_epochs and based on global step
  save_last: true # only work after the first validation step

arcface:
  pth: "artifacts/experiments/idt_encoder/ArcFace.pth"
  vector_size: 256

data:
  trainset_dir: "artifacts/datasets/ffhq/train"
  valset_dir: "artifacts/datasets/ffhq/val"

trainer:
  batch_size: 2

  num_workers: 0
  max_epoch: 100
  num_processes: 1 # for distributed computation
  
  val_check_interval: 800 # control when to valuate model

  limit_train_batches: 1.0 # TODO: review the option
  limit_val_batches: 100 # TODO: review the option

  gpus: -1
  num_sanity_val_steps: 1

  accumulate_grad_batches: 32 # global step depends on the value

  d_per_g_train_ratio: 2 # depends on accumulate_grad_batches and batch_size

model:
  name: "faceshifter"
  experiment: "other_research_code" # to create log and checkpoint dir

  learning_rate_E_G: 1e-4
  learning_rate_D: 1e-4

  beta1: 0
  beta2: 0.999

  grad_clip: 0.0